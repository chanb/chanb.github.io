---
title: 'Deep Q-Learning from Demonstration'
date: 2019-01-30
permalink: /posts/2019/01/dqfd/
tags:
  - Q-Learning
  - Imitation Learning
  - Deep Reinforcement Learning
---

I am currently reading this [Deep Q-Learning from Demonstration](https://arxiv.org/abs/1704.03732) paper for the imitation learning class. Based on my understanding, this paper modifies how we train a DQN by replacing the replay buffer with a prioritized experience replay and changing the loss function to also accommodate for imitating the expert. Additionally, the training process is modified such that there is a pre-training phase and a post-training phase. The pre-training phase is just training the DQN with only expert data inside the replay buffer. The post-training phase is where the agent interacts with the environment and learns from it while enforcing the agent to imitate the expert. Basically the authors are able to get the DQN to perform greatly before any post-training iterations and learn quickly during the post-training iterations.

Background
======

Reinforcement Learning (RL)
------

In short, RL is an area of ML where an agent interacts with an environment, with the goal of maximizing the cumulative reward. The environment is a Markov Decision Process (MDP) consisting: action space, state space, transition function, reward function, and a discount factor. Usually how the process goes is an agent performs an action at the current state, then it observes the next state and obtain the reward. The agent will follow a policy to perform an action. Hence, our goal is to find an optimal policy such that if the agent follows it, the cumulative reward is maximized. There are several ways to do this, but this paper uses Q-Learning.

Q-Learning
------

Deep Q-Learning
------

Double Q-Learning
------

Prioritized Experience Replay
------

Deep Q-Learning from Demonstration
======
